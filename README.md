# ğŸ§â€â™€ï¸ Jetson  Pose Detection â€“ Phase 1 ğŸ¤–  
### *T-Pose Classification using Jetson Inference + SVM*

<p align="center">
  <!-- Badges Row -->
  <img src="https://img.shields.io/badge/Python-3.8-blue?logo=python&logoColor=white" alt="Python 3.8"/>
  <img src="https://img.shields.io/badge/Jetson-Nano-green?logo=nvidia&logoColor=white" alt="Jetson Nano"/>
  <img src="https://img.shields.io/badge/Accuracy-97%25-success?logo=dependabot&logoColor=white" alt="Model Accuracy"/>
  <img src="https://img.shields.io/badge/License-MIT-yellow?logo=open-source-initiative&logoColor=white" alt="License MIT"/>
</p>

<p align="center">
  <img src="pose_data/test_images/Jetson.png" alt="Jetson  Demo" width="720"/>
</p>

**Jetson ** is an edge AI project that leverages **NVIDIA Jetson Inference PoseNet** to detect human body poses and classify them using a Support Vector Machine (SVM).  
This first phase focuses on identifying **T-Pose** vs **Not-T-Pose** using keypoints generated from PoseNet and stored for model training.

---

## ğŸ§© Project Overview

âœ… **PoseNet** â€“ extracts 18 body keypoints from each frame.  
âœ… **CSV Logging** â€“ saves keypoint coordinates & confidence values.  
âœ… **SVM Training** â€“ learns to classify T-Pose vs Not-T-Pose.  
âœ… **Real-Time Prediction** â€“ runs inference with saved `.pkl` models.  

<p align="center">
  <img src="pose_data/test_images/Architecture.png" alt="Jetson  System Architecture" width="800"/>
</p>

---

## âš™ï¸ Folder Structure

```bash
jetson-pose-detection/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pose_keypoints.csv
â”œâ”€â”€ pose_data/
â”‚   â”œâ”€â”€ Tpose/
â”‚   â”œâ”€â”€ Not_Tpose/
â”‚   â””â”€â”€ test_images/
â”‚       â”œâ”€â”€ Architecture.png
â”‚       â”œâ”€â”€ Jetson.png
â”‚       â”œâ”€â”€ Screenshot from 2025-11-04 23-27-56.png
â”‚       â””â”€â”€ Screenshot from 2025-11-04 23-29-28.png
â”œâ”€â”€ svm_model/
â”‚   â”œâ”€â”€ svm_pose_model.pkl
â”‚   â”œâ”€â”€ svm_pose_label_encoder.pkl
â”‚   â””â”€â”€ svm_pose_scaler.pkl
â”œâ”€â”€ Data_collection.py        # Extracts PoseNet keypoints, saves CSV
â”œâ”€â”€ Model_training.py         # Trains SVM model
â”œâ”€â”€ prediction_model.py       # Runs real-time inference
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ§  PoseNet Keypoints Used

```python
KEYPOINTS = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle", "neck"
]
```

Each frame produces `(x, y, confidence)` values for these 18 keypoints.

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Collect Keypoints Data
Run your data collection script to extract keypoints and save them to CSV:

```bash
python3 Data_collection.py
```

Generates:
```
data/pose_keypoints.csv
```

---

### 2ï¸âƒ£ Train the SVM Model
Train the classifier using scikit-learn:
```bash
python3 Model_training.py
```

Saves model files to:
```
svm_model/
 â”œâ”€â”€ svm_pose_model.pkl
 â”œâ”€â”€ svm_pose_label_encoder.pkl
 â””â”€â”€ svm_pose_scaler.pkl
```

---

### 3ï¸âƒ£ Real-Time Pose Classification
Run pose prediction on an image or live input:
```bash
python3 prediction_model.py /path/to/image.png
```

- PoseNet extracts keypoints  
- Scaler normalizes them  
- SVM predicts **T-Pose** or **Not-T-Pose**  
- Outputs prediction + confidence  

---

## ğŸ§¾ Example Jetson Terminal Outputs

### âœ… **T-Pose Detected**
<p align="center">
  <img src="pose_data/test_images/Screenshot from 2025-11-04 23-27-56.png" alt="T-Pose Classification Result" width="800"/>
</p>

### âŒ **Not-T-Pose Detected**
<p align="center">
  <img src="pose_data/test_images/Screenshot from 2025-11-04 23-29-28.png" alt="Not T-Pose Classification Result" width="800"/>
</p>

---

## ğŸ“Š Example CSV Data

| nose_x | nose_y | left_shoulder_x | right_shoulder_x | left_hip_x | right_hip_x | neck_x | neck_y | label |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| 0.421 | 0.331 | 0.312 | 0.632 | 0.310 | 0.621 | 0.470 | 0.320 | Tpose |
| 0.422 | 0.475 | 0.310 | 0.600 | 0.309 | 0.590 | 0.469 | 0.460 | Not_Tpose |

---

## ğŸ“ˆ Model Evaluation

| Metric | Score |
|:--|:--:|
| Accuracy | 0.97 âœ… |
| Precision | 0.95 |
| Recall | 0.96 |
| F1-Score | 0.95 |

---

## ğŸ§­ Roadmap

| Phase | Description | Status |
|:--|:--|:--:|
| 1 | Keypoint Detection + SVM (T-Pose / Not-T-Pose) | âœ… Completed |
| 2 | Multi-Pose Classification (Squat, Plank, Jump) | ğŸš§ In Progress |
| 3 | Real-Time Voice Feedback | ğŸ”œ Planned |
| 4 | Streamlit Dashboard for Analytics | ğŸŒ Future |
| 5 | Edge Optimization (TensorRT) | ğŸ’¡ Upcoming |

---

## ğŸ’¡ Use Cases

- ğŸ§˜â€â™€ï¸ **Fitness & Posture Tracking**  
- ğŸ§‘â€ğŸ« **AI-Assisted Exercise Coaching**  
- ğŸ® **Gesture-Based Controls**  
- ğŸ§  **Human Motion Analytics**

---

## ğŸ§¡ Credits

Developed by **Anjali Jha**  
M.S. in Data Science â€” **University of Maryland, Baltimore County (UMBC)**  

- Edge AI: [NVIDIA Jetson Inference](https://github.com/dusty-nv/jetson-inference)  
- Classifier: [scikit-learn SVM](https://scikit-learn.org/stable/modules/svm.html)  
- Dataset: Captured with Jetson Orin PoseNet inference  

---

## ğŸ“œ License

MIT License Â© 2025 [Anjali Jha](https://github.com/Anjali9815)

---

<p align="center">
  <b>â€œEvery pose is a datapoint toward better movement insight.â€</b>
</p>
